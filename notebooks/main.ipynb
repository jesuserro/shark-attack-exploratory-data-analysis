{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datos cargados correctamente desde ../data/raw/GSAF5.xls\n",
      "‚úÖ Nombres de columnas limpiados.\n",
      "\n",
      "üîπ Valores √∫nicos de 'type' antes de la limpieza:\n",
      "['Unprovoked' 'Provoked' ' Provoked' 'Questionable' 'Watercraft'\n",
      " 'Sea Disaster' nan '?' 'Unconfirmed' 'Unverified' 'Invalid'\n",
      " 'Under investigation' 'Boat']\n",
      "‚úÖ Columna 'type' limpiada.\n",
      "\n",
      "üîπ Valores √∫nicos de 'type' despu√©s de la limpieza:\n",
      "['Unprovoked' 'Provoked' 'Questionable' 'Watercraft' 'Sea Disaster' None\n",
      " 'Boat']\n",
      "\n",
      "üîπ Valores √∫nicos de 'sex' antes de la limpieza:\n",
      "['F' 'M' nan ' M' 'M ' 'lli' 'M x 2' 'N' '.']\n",
      "‚úÖ Columna 'sex' limpiada.\n",
      "\n",
      "üîπ Valores √∫nicos de 'sex' despu√©s de la limpieza:\n",
      "['F' 'M' None]\n",
      "\n",
      "üîπ Valores √∫nicos de 'country' antes de la limpieza:\n",
      "239\n",
      "‚úÖ Columna 'country' limpiada.\n",
      "\n",
      "üîπ Valores √∫nicos de 'country' despu√©s de la limpieza:\n",
      "172\n",
      "\n",
      "üîπ Tama√±o antes de eliminar duplicados: (6994, 23)\n",
      "‚úÖ Tama√±o despu√©s de eliminar duplicados: (6778, 23)\n",
      "‚úÖ Datos limpios guardados en ../data/processed/GSAF5_cleaned.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Turks and Caicos', 'SOUTH AFRICA', 'BAHAMAS', 'USA', 'AUSTRALIA',\n",
       "       'UNITED KINGDOM', 'JAPAN', 'INDONESIA', 'EGYPT', 'JAMAICA',\n",
       "       'BELIZE', 'MALDIVES', 'FRANCE', 'THAILAND', 'COLUMBIA',\n",
       "       'NEW ZEALAND', 'MEXICO', 'COSTA RICA', 'New Zealand', 'BRAZIL',\n",
       "       'British Overseas Territory', 'CANADA', 'ECUADOR', 'JORDAN',\n",
       "       'ST KITTS / NEVIS', 'ST MARTIN', 'SPAIN', 'FIJI', 'SEYCHELLES',\n",
       "       'PAPUA NEW GUINEA', 'ISRAEL', 'CHINA', 'SAMOA', 'IRELAND', 'ITALY',\n",
       "       'COLOMBIA', 'MALAYSIA', 'LIBYA', None, 'CUBA', 'MAURITIUS',\n",
       "       'SOLOMON ISLANDS', 'COMOROS', 'UNITED ARAB EMIRATES',\n",
       "       'PHILIPPINES', 'CAPE VERDE', 'DOMINICAN REPUBLIC', 'NETHERLANDS',\n",
       "       'MOZAMBIQUE', 'PUERTO RICO', 'INTERNATIONAL WATERS', 'GREECE',\n",
       "       'TRINIDAD & TOBAGO', 'KIRIBATI', 'TAIWAN', 'NIGERIA', 'TONGA',\n",
       "       'SCOTLAND', 'CROATIA', 'SAUDI ARABIA', 'CHILE', 'ANTIGUA', 'KENYA',\n",
       "       'RUSSIA', 'PORTUGAL', 'SOUTH KOREA', 'MALTA', 'VIETNAM',\n",
       "       'MADAGASCAR', 'PANAMA', 'SOMALIA', 'NEVIS',\n",
       "       'BRITISH VIRGIN ISLANDS', 'NORWAY', 'SENEGAL', 'YEMEN',\n",
       "       'GULF OF ADEN', 'SIERRA LEONE', 'ST. MAARTIN', 'LIBERIA',\n",
       "       'VANUATU', 'VENEZUELA', 'SRI LANKA', 'URUGUAY', 'INDIA',\n",
       "       'MICRONESIA', 'TANZANIA', 'MARSHALL ISLANDS', 'HONG KONG',\n",
       "       'EL SALVADOR', 'ANGOLA', 'BERMUDA', 'MONTENEGRO', 'IRAN',\n",
       "       'TUNISIA', 'NAMIBIA', 'BANGLADESH', 'PALAU', 'GRENADA', 'IRAQ',\n",
       "       'TURKEY', 'SINGAPORE', 'SUDAN', 'NICARAGUA', 'MALDIVE ISLANDS',\n",
       "       'GABON', 'ARGENTINA', 'MARTINIQUE', 'GUATEMALA', 'JAVA',\n",
       "       'SLOVENIA', 'ICELAND', 'BARBADOS', 'HONDURAS', 'MONACO', 'GUYANA',\n",
       "       'HAITI', 'DOMINIKANA', 'KUWAIT', 'CYPRUS', 'WEST INDIES', 'BURMA',\n",
       "       'LEBANON', 'PARAGUAY', 'BRITISH NEW GUINEA', 'CEYLON', 'OCEAN',\n",
       "       'GEORGIA', 'SYRIA', 'TUVALU', 'INDIAN OCEAN?', 'GUINEA',\n",
       "       'ANDAMAN ISLANDS', 'EQUATORIAL GUINEA / CAMEROON', 'COOK ISLANDS',\n",
       "       'TOBAGO', 'PERU', 'AFRICA', 'ALGERIA', 'Coast of AFRICA',\n",
       "       'TASMAN SEA', 'GHANA', 'GREENLAND', 'MEDITERRANEAN SEA', 'SWEDEN',\n",
       "       'ROATAN', 'Between PORTUGAL & INDIA', 'DJIBOUTI', 'BAHREIN',\n",
       "       'KOREA', 'RED SEA?', 'ASIA?', 'CEYLON (SRI LANKA)'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos m√≥dulos necesarios\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# A√±adimos la ruta de src al path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "# Importamos y recargamos m√≥dulos para evitar cach√© en Jupyter\n",
    "import data_loader\n",
    "import column_cleaner\n",
    "import type_cleaner\n",
    "import sex_cleaner\n",
    "import duplicates_cleaner\n",
    "import country_cleaner\n",
    "\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(column_cleaner)\n",
    "importlib.reload(type_cleaner)\n",
    "importlib.reload(sex_cleaner)\n",
    "importlib.reload(duplicates_cleaner)\n",
    "importlib.reload(country_cleaner)\n",
    "\n",
    "# Importamos las clases\n",
    "from data_loader import DataLoader\n",
    "from column_cleaner import ColumnCleaner\n",
    "from type_cleaner import TypeCleaner\n",
    "from sex_cleaner import SexCleaner\n",
    "from duplicates_cleaner import DuplicatesCleaner\n",
    "from country_cleaner import CountryCleaner\n",
    "\n",
    "# Ruta de entrada y salida\n",
    "input_file = \"../data/raw/GSAF5.xls\"\n",
    "output_file = \"../data/processed/GSAF5_cleaned.xlsx\"\n",
    "\n",
    "# 1Ô∏è‚É£ Cargar los datos\n",
    "loader = DataLoader(input_file)\n",
    "loader.load_data()\n",
    "df_original = loader.get_data()  # Guardamos el DataFrame original\n",
    "\n",
    "# 2Ô∏è‚É£ Limpiar los nombres de las columnas\n",
    "column_cleaner = ColumnCleaner(df_original)\n",
    "column_cleaner.clean_columns()\n",
    "df_cleaned = column_cleaner.get_cleaned_data()  # DataFrame con nombres limpios\n",
    "\n",
    "# 3Ô∏è‚É£ Mostrar valores √∫nicos de la columna \"type\" antes de la limpieza\n",
    "if \"type\" in df_cleaned.columns:\n",
    "    print(\"\\nüîπ Valores √∫nicos de 'type' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"type\"].unique())\n",
    "\n",
    "# 3Ô∏è‚É£ Limpiar la columna \"type\"\n",
    "type_cleaner = TypeCleaner(df_cleaned)\n",
    "type_cleaner.clean_type_column()\n",
    "df_cleaned = type_cleaner.get_cleaned_data()  # DataFrame con \"type\" limpio\n",
    "\n",
    "# 5Ô∏è‚É£ Mostrar valores √∫nicos de 'type' despu√©s de la limpieza\n",
    "if \"type\" in df_cleaned.columns:\n",
    "    print(\"\\nüîπ Valores √∫nicos de 'type' despu√©s de la limpieza:\")\n",
    "    print(df_cleaned[\"type\"].unique())\n",
    "\n",
    "# 6Ô∏è‚É£ Mostrar valores √∫nicos de \"sex\" antes de la limpieza\n",
    "if \"sex\" in df_cleaned.columns:\n",
    "    print(\"\\nüîπ Valores √∫nicos de 'sex' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"sex\"].unique())\n",
    "\n",
    "# 4Ô∏è‚É£ Limpiar la columna \"sex\"\n",
    "sex_cleaner = SexCleaner(df_cleaned)\n",
    "sex_cleaner.clean_sex_column()\n",
    "df_cleaned = sex_cleaner.get_cleaned_data()  # DataFrame con \"sex\" limpio\n",
    "\n",
    "# 8Ô∏è‚É£ Mostrar valores √∫nicos de \"sex\" despu√©s de la limpieza\n",
    "if \"sex\" in df_cleaned.columns:\n",
    "    print(\"\\nüîπ Valores √∫nicos de 'sex' despu√©s de la limpieza:\")\n",
    "    print(df_cleaned[\"sex\"].unique())\n",
    "\n",
    "# 9Ô∏è‚É£ Mostrar valores √∫nicos de \"country\" antes de la limpieza\n",
    "if \"country\" in df_cleaned.columns:\n",
    "    print(\"\\nüîπ Valores √∫nicos de 'country' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"country\"].nunique())\n",
    "\n",
    "# 7Ô∏è‚É£ Limpiar la columna \"country\"\n",
    "country_cleaner = CountryCleaner(df_cleaned)\n",
    "country_cleaner.clean_country_column()\n",
    "df_cleaned = country_cleaner.get_cleaned_data()  # DataFrame con \"country\" limpio\n",
    "\n",
    "# 10Ô∏è‚É£ Mostrar valores √∫nicos de 'country' despu√©s de la limpieza\n",
    "if \"country\" in df_cleaned.columns:\n",
    "    print(\"\\nüîπ Valores √∫nicos de 'country' despu√©s de la limpieza:\")\n",
    "    print(df_cleaned[\"country\"].nunique())\n",
    "\n",
    "# 5Ô∏è‚É£ Eliminar duplicados en \"case_number\"\n",
    "duplicates_cleaner = DuplicatesCleaner(df_cleaned)\n",
    "duplicates_cleaner.remove_duplicates()\n",
    "df_final = duplicates_cleaner.get_cleaned_data()  # DataFrame sin duplicados\n",
    "\n",
    "# 6Ô∏è‚É£ Guardar los datos procesados con nombres de columnas, \"type\", \"sex\", \"country\" y sin duplicados\n",
    "duplicates_cleaner.save_cleaned_data(output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
