{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos cargados correctamente desde ../data/raw/GSAF5.xls\n",
      "✅ Nombres de columnas limpiados.\n",
      "\n",
      "🔹 Valores únicos de 'type' antes de la limpieza:\n",
      "['Unprovoked' 'Provoked' ' Provoked' 'Questionable' 'Watercraft'\n",
      " 'Sea Disaster' nan '?' 'Unconfirmed' 'Unverified' 'Invalid'\n",
      " 'Under investigation' 'Boat']\n",
      "✅ Columna 'type' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'type' después de la limpieza:\n",
      "['Unprovoked' 'Provoked' 'Questionable' 'Watercraft' 'Sea Disaster' None\n",
      " 'Boat']\n",
      "\n",
      "🔹 Valores únicos de 'sex' antes de la limpieza:\n",
      "['F' 'M' nan ' M' 'M ' 'lli' 'M x 2' 'N' '.']\n",
      "✅ Columna 'sex' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'sex' después de la limpieza:\n",
      "['F' 'M' None]\n",
      "\n",
      "🔹 Valores únicos de 'country' antes de la limpieza:\n",
      "239\n",
      "✅ Columna 'country' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'country' después de la limpieza:\n",
      "172\n",
      "\n",
      "🔹 Valores únicos de 'fatal_y/n' antes de la limpieza:\n",
      "['N' 'Y' 'F' 'M' nan 'n' 'Nq' 'UNKNOWN' 2017 'Y x 2' ' N' 'N ' 'y']\n",
      "✅ Columna 'fatal' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'fatal' después de la limpieza:\n",
      "[0. 1.]\n",
      "\n",
      "🔹 Valores únicos de 'time' antes de la limpieza:\n",
      "436\n",
      "✅ Columna 'time' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'time' después de la limpieza:\n",
      "4\n",
      "\n",
      "🔹 Valores únicos de 'species' antes de la limpieza:\n",
      "['Unknown' 'Bull shark' 'Not stated' ... \"12' tiger shark\" 'Blue pointers'\n",
      " 'Said to involve a grey nurse shark that leapt out of the water and  seized the boy but species identification is questionable']\n",
      "✅ Columna 'species' limpiada y valores nulos y 'Unknown' rellenados con la moda.\n",
      "\n",
      "🔹 Valores únicos de 'species' después de la limpieza:\n",
      "['Large' 'Small' 'Medium']\n",
      "\n",
      "🔹 Valores únicos de 'year' antes de la limpieza:\n",
      "261\n",
      "✅ Columna 'year' limpiada y valores nulos rellenados con la media.\n",
      "\n",
      "🔹 Valores únicos de 'year' después de la limpieza:\n",
      "258\n",
      "\n",
      "🔹 Valores únicos de 'activity' antes de la limpieza:\n",
      "['Swimming' 'Bathing' 'Surfing' ...\n",
      " 'Crew swimming alongside their anchored ship' '4 men were bathing'\n",
      " 'Wreck of  large double sailing canoe']\n",
      "✅ Columna 'activity' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'activity' después de la limpieza:\n",
      "['Swimming' 'Bathing' 'Surfing' ...\n",
      " 'Crew swimming alongside their anchored ship' '4 men were bathing'\n",
      " 'Wreck of  large double sailing canoe']\n",
      "\n",
      "🔹 Tamaño antes de eliminar duplicados: (6994, 23)\n",
      "✅ Tamaño después de eliminar duplicados: (6778, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesus\\Documents\\Ironhack\\DataAnalyticsFeb2025\\Unit 2 - DW & Retrieval\\shark-attacks\\src\\species_cleaner.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.cleaned_df[\"species\"].fillna(mode_value, inplace=True)\n",
      "c:\\Users\\jesus\\Documents\\Ironhack\\DataAnalyticsFeb2025\\Unit 2 - DW & Retrieval\\shark-attacks\\src\\year_cleaner.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.cleaned_df[\"year\"].fillna(mean_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos limpios guardados en ../data/processed/GSAF5_cleaned.xlsx\n",
      "|     | date                |   year | type       | country          | state                 | location                       | activity                | name               | sex   |   age | injury                                                            |   fatal | time    | species   | source                      | pdf                          | href_formula                                                                       | href                                                                               | case_number   | case_number.1   |   original_order |   unnamed:_21 |   unnamed:_22 |\n",
      "|----:|:--------------------|-------:|:-----------|:-----------------|:----------------------|:-------------------------------|:------------------------|:-------------------|:------|------:|:------------------------------------------------------------------|--------:|:--------|:----------|:----------------------------|:-----------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------|:----------------|-----------------:|--------------:|--------------:|\n",
      "|   0 | 2025-02-07 00:00:00 |   2025 | Unprovoked | Turks and Caicos | nan                   | Thompson Cove Beach            | Swimming                | Unknown            | F     |    55 | Unknown                                                           |       0 | Unknown | Large     | Todd Smith: Platform X      | nan                          | nan                                                                                | nan                                                                                | nan           | nan             |              nan |           nan |           nan |\n",
      "| 187 | 25-Sep-2022         |   2022 | Unprovoked | SOUTH AFRICA     | Western Cape Province | Central Beach, Plettenberg Bay | Swimming                | Kimon Bisogno      | F     |    39 | FATAL                                                             |       1 | M       | Large     | Mirror, 9/25/2022           | 2022.09.25-Plett.pdf         | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.25-Plett.pdf         | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.25-Plett.pdf         | 2022.09.25    | 2022.09.25      |             6802 |           nan |           nan |\n",
      "| 189 | 06-Sep-2022         |   2022 | Unprovoked | BAHAMAS          | nan                   | Green Cay                      | Snorkeling              | Caroline DiPlacido | F     |    58 | FATAL                                                             |       1 | T       | Large     | B. Myatt, GSAF              | 2022.09.06-Bahamas.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.06-Bahamas.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.06-Bahamas.pdf       | 2022.09.06    | 2022.09.06      |             6801 |           nan |           nan |\n",
      "| 190 | 03-Sep-2022         |   2022 | Unprovoked | USA              | Hawaii                | Lower Paia Beach Park, Maui    | Swimming  or Snorkeling | female             | F     |    51 | Injuries to left arm and right hand                               |       0 | T       | Large     | Star Advertiser, 9/3/2022   | 2022.09.03-Maui.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.03-Maui.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.03-Maui.pdf          | 2022.09.03    | 2022.09.03      |             6800 |           nan |           nan |\n",
      "| 191 | 31-Aug-2022         |   2022 | Unprovoked | AUSTRALIA        | New South Wales       | Avoca                          | Surfing                 | Sunni Pace         | M     |    14 | Puncture wounds to right hand & forearm                           |       0 | M       | Large     | Surfline, 9/2/2022          | 2022.08.31-Pace.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.31-Pace.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.31-Pace.pdf          | 2022.08.31    | 2022.08.31      |             6799 |           nan |           nan |\n",
      "| 192 | 17-Aug-2022         |   2022 | Unprovoked | AUSTRALIA        | New South Wales       | Coffs Harbour                  | Kayaking                | John Vincent       | M     |   nan | No injury, kayak bitten in half                                   |       0 | Unknown | Large     | A Currie. GSAF              | 2022.08.17-CoffsHarbour.pdf  | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.17-CoffsHarbour.pdf  | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.17-CoffsHarbour      | 2022.08.17    | 2022.08.17      |             6798 |           nan |           nan |\n",
      "| 193 | 15-Aug-2022         |   2022 | Unprovoked | USA              | South Carolina        | Myrtle Beach, Horry County     | Swimming                | female             | F     |   nan | Minor injury to leg                                               |       0 | M       | Large     | C. Creswell, GSAF           | 2022.08.15.c-MyrtleBeach.pdf | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.c-MyrtleBeach.pdf | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.c-MyrtleBeach.pdf | 2022.08.15.c  | 2022.08.15.c    |             6797 |           nan |           nan |\n",
      "| 194 | 15-Aug-2022         |   2022 | Unprovoked | USA              | South Carolina        | Myrtle Beach, Horry County     | Standing                | Karrren Sites      | F     |   nan | Multiple lLacerations to right forearm                            |       0 | T       | Medium    | C. Creswell, GSAF           | 2022.08.15.b-Sites.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.b-Sites.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.b-Sites.pdf       | 2022.08.15.b  | 2022.08.15.b    |             6796 |           nan |           nan |\n",
      "| 195 | 15-Aug-2022         |   2022 | Unprovoked | AUSTRALIA        | Westerm Australia     | Goode Beach                    | Spearfishing            | Luke Pasco         | M     |    17 | Lacerations to lower legs                                         |       0 | Unknown | Large     | S. De March, GSAF           | 2022.08.15.a-Pasco.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.a-Pasco.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.a-Pasco.pdf       | 2022.08.15.a  | 2022.08.15.a    |             6795 |           nan |           nan |\n",
      "| 196 | 13-Aug-2022         |   2022 | Unprovoked | USA              | Florida               | Looe Key, Monroe County        | Snorkeling              | Jameson Reeder Jr, | M     |    10 | Lower left leg severely bitten, necessitating surgical amputation |       0 | T       | Large     | Orlando Sentinel, 8/15/2022 | 2022.08.13-Reeder.pdf        | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.13-Reeder.pdf        | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.13-Reeder.pdf        | 2022.08.13    | 2022.08.13      |             6794 |           nan |           nan |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>fatal</th>\n",
       "      <th>original_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6778.000000</td>\n",
       "      <td>6778.000000</td>\n",
       "      <td>6777.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1970.586309</td>\n",
       "      <td>0.211862</td>\n",
       "      <td>3399.289509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>50.231287</td>\n",
       "      <td>0.408658</td>\n",
       "      <td>1962.234774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1701.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1983.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2008.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5097.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2025.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6802.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              year        fatal  original_order\n",
       "count  6778.000000  6778.000000     6777.000000\n",
       "mean   1970.586309     0.211862     3399.289509\n",
       "std      50.231287     0.408658     1962.234774\n",
       "min    1000.000000     0.000000        2.000000\n",
       "25%    1950.000000     0.000000     1701.000000\n",
       "50%    1983.000000     0.000000     3400.000000\n",
       "75%    2008.000000     0.000000     5097.000000\n",
       "max    2025.000000     1.000000     6802.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importamos módulos necesarios\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# Añadimos la ruta de src al path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "# Importamos y recargamos módulos para evitar caché en Jupyter\n",
    "import data_loader\n",
    "import column_cleaner\n",
    "import type_cleaner\n",
    "import sex_cleaner\n",
    "import duplicates_cleaner\n",
    "import country_cleaner\n",
    "import fatal_cleaner\n",
    "import time_cleaner\n",
    "import species_cleaner\n",
    "import year_cleaner\n",
    "import activity_cleaner\n",
    "\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(column_cleaner)\n",
    "importlib.reload(type_cleaner)\n",
    "importlib.reload(sex_cleaner)\n",
    "importlib.reload(duplicates_cleaner)\n",
    "importlib.reload(country_cleaner)\n",
    "importlib.reload(fatal_cleaner)\n",
    "importlib.reload(time_cleaner)\n",
    "importlib.reload(species_cleaner)\n",
    "importlib.reload(year_cleaner)\n",
    "importlib.reload(activity_cleaner)\n",
    "\n",
    "# Importamos las clases\n",
    "from data_loader import DataLoader\n",
    "from column_cleaner import ColumnCleaner\n",
    "from type_cleaner import TypeCleaner\n",
    "from sex_cleaner import SexCleaner\n",
    "from duplicates_cleaner import DuplicatesCleaner\n",
    "from country_cleaner import CountryCleaner\n",
    "from fatal_cleaner import FatalCleaner\n",
    "from time_cleaner import TimeCleaner\n",
    "from species_cleaner import SpeciesCleaner\n",
    "from year_cleaner import YearCleaner\n",
    "from activity_cleaner import ActivityCleaner\n",
    "\n",
    "# Ruta de entrada y salida\n",
    "input_file = \"../data/raw/GSAF5.xls\"\n",
    "output_file = \"../data/processed/GSAF5_cleaned.xlsx\"\n",
    "\n",
    "# 1️⃣ Cargar los datos\n",
    "loader = DataLoader(input_file)\n",
    "loader.load_data()\n",
    "df_original = loader.get_data()  # Guardamos el DataFrame original\n",
    "\n",
    "# 2️⃣ Limpiar los nombres de las columnas\n",
    "column_cleaner = ColumnCleaner(df_original)\n",
    "column_cleaner.clean_columns()\n",
    "df_cleaned = column_cleaner.get_cleaned_data()  # DataFrame con nombres limpios\n",
    "\n",
    "# 3️⃣ Mostrar valores únicos de la columna \"type\" antes de la limpieza\n",
    "if \"type\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'type' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"type\"].unique())\n",
    "\n",
    "# 4️⃣ Limpiar la columna \"type\"\n",
    "type_cleaner = TypeCleaner(df_cleaned)\n",
    "type_cleaner.clean_type_column()\n",
    "df_cleaned = type_cleaner.get_cleaned_data()  # DataFrame con \"type\" limpio\n",
    "\n",
    "# 5️⃣ Mostrar valores únicos de 'type' después de la limpieza\n",
    "if \"type\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'type' después de la limpieza:\")\n",
    "    print(df_cleaned[\"type\"].unique())\n",
    "\n",
    "# 6️⃣ Mostrar valores únicos de \"sex\" antes de la limpieza\n",
    "if \"sex\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'sex' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"sex\"].unique())\n",
    "\n",
    "# 7️⃣ Limpiar la columna \"sex\"\n",
    "sex_cleaner = SexCleaner(df_cleaned)\n",
    "sex_cleaner.clean_sex_column()\n",
    "df_cleaned = sex_cleaner.get_cleaned_data()  # DataFrame con \"sex\" limpio\n",
    "\n",
    "# 8️⃣ Mostrar valores únicos de \"sex\" después de la limpieza\n",
    "if \"sex\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'sex' después de la limpieza:\")\n",
    "    print(df_cleaned[\"sex\"].unique())\n",
    "\n",
    "# 9️⃣ Mostrar valores únicos de \"country\" antes de la limpieza\n",
    "if \"country\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'country' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"country\"].nunique())\n",
    "\n",
    "# 🔟 Limpiar la columna \"country\"\n",
    "country_cleaner = CountryCleaner(df_cleaned)\n",
    "country_cleaner.clean_country_column()\n",
    "df_cleaned = country_cleaner.get_cleaned_data()  # DataFrame con \"country\" limpio\n",
    "\n",
    "# 1️⃣1️⃣ Mostrar valores únicos de 'country' después de la limpieza\n",
    "if \"country\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'country' después de la limpieza:\")\n",
    "    print(df_cleaned[\"country\"].nunique())\n",
    "\n",
    "# 1️⃣2️⃣ Mostrar valores únicos de \"fatal_y/n\" antes de la limpieza\n",
    "if \"fatal_y/n\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'fatal_y/n' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"fatal_y/n\"].unique())\n",
    "\n",
    "# 1️⃣3️⃣ Limpiar la columna \"fatal_y/n\"\n",
    "fatal_cleaner = FatalCleaner(df_cleaned)\n",
    "fatal_cleaner.clean_fatal_column()\n",
    "df_cleaned = fatal_cleaner.get_cleaned_data()  # DataFrame con \"fatal\" limpio\n",
    "\n",
    "# 1️⃣4️⃣ Mostrar valores únicos de 'fatal' después de la limpieza\n",
    "if \"fatal\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'fatal' después de la limpieza:\")\n",
    "    print(df_cleaned[\"fatal\"].unique())\n",
    "\n",
    "# 1️⃣5️⃣ Mostrar valores únicos de \"time\" antes de la limpieza\n",
    "if \"time\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'time' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"time\"].nunique())\n",
    "\n",
    "# 1️⃣6️⃣ Limpiar la columna \"time\"\n",
    "time_cleaner = TimeCleaner(df_cleaned)\n",
    "time_cleaner.clean_time_column()\n",
    "df_cleaned = time_cleaner.get_cleaned_data()  # DataFrame con \"time\" limpio\n",
    "\n",
    "# 1️⃣7️⃣ Mostrar valores únicos de 'time' después de la limpieza\n",
    "if \"time\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'time' después de la limpieza:\")\n",
    "    print(df_cleaned[\"time\"].nunique())\n",
    "\n",
    "# 1️⃣8️⃣ Mostrar valores únicos de \"species\" antes de la limpieza\n",
    "if \"species\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'species' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"species\"].unique())\n",
    "\n",
    "# 1️⃣9️⃣ Limpiar la columna \"species\"\n",
    "species_cleaner = SpeciesCleaner(df_cleaned)\n",
    "species_cleaner.clean_species_column()\n",
    "df_cleaned = species_cleaner.get_cleaned_data()  # DataFrame con \"species\" limpio\n",
    "\n",
    "# 2️⃣0️⃣ Mostrar valores únicos de 'species' después de la limpieza\n",
    "if \"species\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'species' después de la limpieza:\")\n",
    "    print(df_cleaned[\"species\"].unique())\n",
    "\n",
    "# 2️⃣1️⃣ Mostrar valores únicos de \"year\" antes de la limpieza\n",
    "if \"year\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'year' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"year\"].nunique())\n",
    "\n",
    "# 2️⃣2️⃣ Limpiar la columna \"year\"\n",
    "year_cleaner = YearCleaner(df_cleaned)\n",
    "year_cleaner.clean_year_column()\n",
    "df_cleaned = year_cleaner.get_cleaned_data()  # DataFrame con \"year\" limpio\n",
    "\n",
    "# 2️⃣3️⃣ Mostrar valores únicos de 'year' después de la limpieza\n",
    "if \"year\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'year' después de la limpieza:\")\n",
    "    print(df_cleaned[\"year\"].nunique())\n",
    "\n",
    "# 2️⃣4️⃣ Mostrar valores únicos de \"activity\" antes de la limpieza\n",
    "if \"activity\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'activity' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"activity\"].unique())\n",
    "\n",
    "# 2️⃣5️⃣ Limpiar la columna \"activity\"\n",
    "activity_cleaner = ActivityCleaner(df_cleaned)\n",
    "activity_cleaner.clean_activity_column()\n",
    "df_cleaned = activity_cleaner.get_cleaned_data()  # DataFrame con \"activity\" limpio\n",
    "\n",
    "# 2️⃣6️⃣ Mostrar valores únicos de 'activity' después de la limpieza\n",
    "if \"activity\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'activity' después de la limpieza:\")\n",
    "    print(df_cleaned[\"activity\"].unique())\n",
    "\n",
    "# 2️⃣7️⃣ Eliminar duplicados en \"case_number\"\n",
    "duplicates_cleaner = DuplicatesCleaner(df_cleaned)\n",
    "duplicates_cleaner.remove_duplicates()\n",
    "df_final = duplicates_cleaner.get_cleaned_data()  # DataFrame sin duplicados\n",
    "\n",
    "# 2️⃣8️⃣ Guardar los datos procesados con nombres de columnas, \"type\", \"sex\", \"country\", \"fatal\", \"time\", \"species\", \"year\" y sin duplicados\n",
    "duplicates_cleaner.save_cleaned_data(output_file)\n",
    "\n",
    "markdown_table = df_final.head(10).to_markdown()\n",
    "print(markdown_table)\n",
    "\n",
    "# REPASO FINAL:\n",
    "# - Comprobamos el tipo de datos\n",
    "# df_final.info()\n",
    "\n",
    "# df_final.describe(include='object').T\n",
    "\n",
    "# df_final.select_dtypes(include='object')\n",
    "\n",
    "# - Comprobamos las columnas con valores nulos\n",
    "# df_final.isnull().sum()\n",
    "# df_final.isnull().sum()/df_final.shape[0] * 100 # Porcentaje de valores nulos es importante\n",
    "\n",
    "# - Comprobación de duplicados\n",
    "# df_final.duplicated().sum()/df_final.shape[0] * 100\n",
    "# Mirar combinaciones de columnas duplicadas\n",
    "# df_final.duplicated(subset=['ticket', 'id_Cliente']).sum()\n",
    "\n",
    "# - Formateo de datos\n",
    "# - a. Cambiar tipo de datos\n",
    "# df_final['year'] = df_final['year'].astype(int)\n",
    "\n",
    "# - b. Limpiar datos\n",
    "# round(), format, string.lower()\n",
    "\n",
    "# - apply() operaciones por columna\n",
    "# - map()\n",
    "# - applymap()\n",
    "\n",
    "# - Filtrado de datos\n",
    "# - Aconseja crear nuevos dataframes con cada filtro para conservar el original\n",
    "# - Filtrado por columnas combinado\n",
    "\n",
    "# poner indice en la columna que más se va a usar para filtrar\n",
    "# df_final.set_index('passengerID', inplace=True)\n",
    "\n",
    "# Crear nueva columna significativa\n",
    "# df_final['año_nacimiento'] = 2025 - df_final['Age']\n",
    "# Y Eliminar columna 'Age'\n",
    "# df_final.drop(columns='Age', inplace=True)\n",
    "\n",
    "# Mezclas de dataframes\n",
    "# - 1 Concatenar (une de forma vertical por filas) Es menos usado.\n",
    "# df_final = pd.concat([df1, df2], axis=0)\n",
    "# - 2 Merge (une de forma horizontal por columnas a través de una columna común. Lo usaremos más)\n",
    "# df_final = pd.merge(df1, df2, on='columna_comun' how='inner')\n",
    "# df_final = pd.merge(df1, df2, left_on='columna1', right_on='columna2', how='inner') # Si las columnas no tienen el mismo nombre en ambos dataframes\n",
    "# - 3 Join: une por índices (no tan usado)\n",
    "\n",
    "# Pivot Table: Cambia la forma de la tabla: filas a columnas y viceversa\n",
    "# df_final.pivot_table(index='country', columns='year', values='GDP', aggfunc='sum')\n",
    "\n",
    "# Agregaciones: análisis por grupos del dataframe\n",
    "# df_final.groupby('country')['GDP'].sum()\n",
    "# df_final.groupby('sex', 'survived')['age'].mean()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
