{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos cargados correctamente desde ../data/raw/GSAF5.xls\n",
      "✅ Nombres de columnas limpiados.\n",
      "\n",
      "🔹 Valores únicos de 'type' antes de la limpieza:\n",
      "['Unprovoked' 'Provoked' ' Provoked' 'Questionable' 'Watercraft'\n",
      " 'Sea Disaster' nan '?' 'Unconfirmed' 'Unverified' 'Invalid'\n",
      " 'Under investigation' 'Boat']\n",
      "✅ Columna 'type' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'type' después de la limpieza:\n",
      "['Unprovoked' 'Provoked' 'Questionable' 'Watercraft' 'Sea Disaster' None\n",
      " 'Boat']\n",
      "\n",
      "🔹 Valores únicos de 'sex' antes de la limpieza:\n",
      "['F' 'M' nan ' M' 'M ' 'lli' 'M x 2' 'N' '.']\n",
      "✅ Columna 'sex' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'sex' después de la limpieza:\n",
      "['F' 'M' None]\n",
      "\n",
      "🔹 Valores únicos de 'country' antes de la limpieza:\n",
      "239\n",
      "✅ Columna 'country' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'country' después de la limpieza:\n",
      "172\n",
      "\n",
      "🔹 Valores únicos de 'fatal_y/n' antes de la limpieza:\n",
      "['N' 'Y' 'F' 'M' nan 'n' 'Nq' 'UNKNOWN' 2017 'Y x 2' ' N' 'N ' 'y']\n",
      "✅ Columna 'fatal' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'fatal' después de la limpieza:\n",
      "[0. 1.]\n",
      "\n",
      "🔹 Valores únicos de 'time' antes de la limpieza:\n",
      "436\n",
      "✅ Columna 'time' limpiada.\n",
      "\n",
      "🔹 Valores únicos de 'time' después de la limpieza:\n",
      "4\n",
      "\n",
      "🔹 Valores únicos de 'species' antes de la limpieza:\n",
      "['Unknown' 'Bull shark' 'Not stated' ... \"12' tiger shark\" 'Blue pointers'\n",
      " 'Said to involve a grey nurse shark that leapt out of the water and  seized the boy but species identification is questionable']\n",
      "✅ Columna 'species' limpiada y valores nulos y 'Unknown' rellenados con la moda.\n",
      "\n",
      "🔹 Valores únicos de 'species' después de la limpieza:\n",
      "['Large' 'Small' 'Medium']\n",
      "\n",
      "🔹 Tamaño antes de eliminar duplicados: (6994, 23)\n",
      "✅ Tamaño después de eliminar duplicados: (6778, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesus\\Documents\\Ironhack\\DataAnalyticsFeb2025\\Unit 2 - DW & Retrieval\\shark-attacks\\src\\species_cleaner.py:113: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Datos limpios guardados en ../data/processed/GSAF5_cleaned.xlsx\n",
      "|     | date                |   year | type       | country          | state                 | location                       | activity                | name               | sex   |   age | injury                                                            |   fatal | time    | species   | source                      | pdf                          | href_formula                                                                       | href                                                                               | case_number   | case_number.1   |   original_order |   unnamed:_21 |   unnamed:_22 |\n",
      "|----:|:--------------------|-------:|:-----------|:-----------------|:----------------------|:-------------------------------|:------------------------|:-------------------|:------|------:|:------------------------------------------------------------------|--------:|:--------|:----------|:----------------------------|:-----------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------|:----------------|-----------------:|--------------:|--------------:|\n",
      "|   0 | 2025-02-07 00:00:00 |   2025 | Unprovoked | Turks and Caicos | nan                   | Thompson Cove Beach            | Swimming                | Unknown            | F     |    55 | Unknown                                                           |       0 | Unknown | Large     | Todd Smith: Platform X      | nan                          | nan                                                                                | nan                                                                                | nan           | nan             |              nan |           nan |           nan |\n",
      "| 187 | 25-Sep-2022         |   2022 | Unprovoked | SOUTH AFRICA     | Western Cape Province | Central Beach, Plettenberg Bay | Swimming                | Kimon Bisogno      | F     |    39 | FATAL                                                             |       1 | M       | Large     | Mirror, 9/25/2022           | 2022.09.25-Plett.pdf         | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.25-Plett.pdf         | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.25-Plett.pdf         | 2022.09.25    | 2022.09.25      |             6802 |           nan |           nan |\n",
      "| 189 | 06-Sep-2022         |   2022 | Unprovoked | BAHAMAS          | nan                   | Green Cay                      | Snorkeling              | Caroline DiPlacido | F     |    58 | FATAL                                                             |       1 | T       | Large     | B. Myatt, GSAF              | 2022.09.06-Bahamas.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.06-Bahamas.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.06-Bahamas.pdf       | 2022.09.06    | 2022.09.06      |             6801 |           nan |           nan |\n",
      "| 190 | 03-Sep-2022         |   2022 | Unprovoked | USA              | Hawaii                | Lower Paia Beach Park, Maui    | Swimming  or Snorkeling | female             | F     |    51 | Injuries to left arm and right hand                               |       0 | T       | Large     | Star Advertiser, 9/3/2022   | 2022.09.03-Maui.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.03-Maui.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.09.03-Maui.pdf          | 2022.09.03    | 2022.09.03      |             6800 |           nan |           nan |\n",
      "| 191 | 31-Aug-2022         |   2022 | Unprovoked | AUSTRALIA        | New South Wales       | Avoca                          | Surfing                 | Sunni Pace         | M     |    14 | Puncture wounds to right hand & forearm                           |       0 | M       | Large     | Surfline, 9/2/2022          | 2022.08.31-Pace.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.31-Pace.pdf          | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.31-Pace.pdf          | 2022.08.31    | 2022.08.31      |             6799 |           nan |           nan |\n",
      "| 192 | 17-Aug-2022         |   2022 | Unprovoked | AUSTRALIA        | New South Wales       | Coffs Harbour                  | Kayaking                | John Vincent       | M     |   nan | No injury, kayak bitten in half                                   |       0 | Unknown | Large     | A Currie. GSAF              | 2022.08.17-CoffsHarbour.pdf  | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.17-CoffsHarbour.pdf  | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.17-CoffsHarbour      | 2022.08.17    | 2022.08.17      |             6798 |           nan |           nan |\n",
      "| 193 | 15-Aug-2022         |   2022 | Unprovoked | USA              | South Carolina        | Myrtle Beach, Horry County     | Swimming                | female             | F     |   nan | Minor injury to leg                                               |       0 | M       | Large     | C. Creswell, GSAF           | 2022.08.15.c-MyrtleBeach.pdf | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.c-MyrtleBeach.pdf | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.c-MyrtleBeach.pdf | 2022.08.15.c  | 2022.08.15.c    |             6797 |           nan |           nan |\n",
      "| 194 | 15-Aug-2022         |   2022 | Unprovoked | USA              | South Carolina        | Myrtle Beach, Horry County     | Standing                | Karrren Sites      | F     |   nan | Multiple lLacerations to right forearm                            |       0 | T       | Medium    | C. Creswell, GSAF           | 2022.08.15.b-Sites.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.b-Sites.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.b-Sites.pdf       | 2022.08.15.b  | 2022.08.15.b    |             6796 |           nan |           nan |\n",
      "| 195 | 15-Aug-2022         |   2022 | Unprovoked | AUSTRALIA        | Westerm Australia     | Goode Beach                    | Spearfishing            | Luke Pasco         | M     |    17 | Lacerations to lower legs                                         |       0 | Unknown | Large     | S. De March, GSAF           | 2022.08.15.a-Pasco.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.a-Pasco.pdf       | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.15.a-Pasco.pdf       | 2022.08.15.a  | 2022.08.15.a    |             6795 |           nan |           nan |\n",
      "| 196 | 13-Aug-2022         |   2022 | Unprovoked | USA              | Florida               | Looe Key, Monroe County        | Snorkeling              | Jameson Reeder Jr, | M     |    10 | Lower left leg severely bitten, necessitating surgical amputation |       0 | T       | Large     | Orlando Sentinel, 8/15/2022 | 2022.08.13-Reeder.pdf        | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.13-Reeder.pdf        | http://sharkattackfile.net/spreadsheets/pdf_directory/2022.08.13-Reeder.pdf        | 2022.08.13    | 2022.08.13      |             6794 |           nan |           nan |\n"
     ]
    }
   ],
   "source": [
    "# Importamos módulos necesarios\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# Añadimos la ruta de src al path\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "# Importamos y recargamos módulos para evitar caché en Jupyter\n",
    "import data_loader\n",
    "import column_cleaner\n",
    "import type_cleaner\n",
    "import sex_cleaner\n",
    "import duplicates_cleaner\n",
    "import country_cleaner\n",
    "import fatal_cleaner\n",
    "import time_cleaner\n",
    "import species_cleaner\n",
    "\n",
    "importlib.reload(data_loader)\n",
    "importlib.reload(column_cleaner)\n",
    "importlib.reload(type_cleaner)\n",
    "importlib.reload(sex_cleaner)\n",
    "importlib.reload(duplicates_cleaner)\n",
    "importlib.reload(country_cleaner)\n",
    "importlib.reload(fatal_cleaner)\n",
    "importlib.reload(time_cleaner)\n",
    "importlib.reload(species_cleaner)\n",
    "\n",
    "# Importamos las clases\n",
    "from data_loader import DataLoader\n",
    "from column_cleaner import ColumnCleaner\n",
    "from type_cleaner import TypeCleaner\n",
    "from sex_cleaner import SexCleaner\n",
    "from duplicates_cleaner import DuplicatesCleaner\n",
    "from country_cleaner import CountryCleaner\n",
    "from fatal_cleaner import FatalCleaner\n",
    "from time_cleaner import TimeCleaner\n",
    "from species_cleaner import SpeciesCleaner\n",
    "\n",
    "# Ruta de entrada y salida\n",
    "input_file = \"../data/raw/GSAF5.xls\"\n",
    "output_file = \"../data/processed/GSAF5_cleaned.xlsx\"\n",
    "\n",
    "# 1️⃣ Cargar los datos\n",
    "loader = DataLoader(input_file)\n",
    "loader.load_data()\n",
    "df_original = loader.get_data()  # Guardamos el DataFrame original\n",
    "\n",
    "# 2️⃣ Limpiar los nombres de las columnas\n",
    "column_cleaner = ColumnCleaner(df_original)\n",
    "column_cleaner.clean_columns()\n",
    "df_cleaned = column_cleaner.get_cleaned_data()  # DataFrame con nombres limpios\n",
    "\n",
    "# 3️⃣ Mostrar valores únicos de la columna \"type\" antes de la limpieza\n",
    "if \"type\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'type' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"type\"].unique())\n",
    "\n",
    "# 3️⃣ Limpiar la columna \"type\"\n",
    "type_cleaner = TypeCleaner(df_cleaned)\n",
    "type_cleaner.clean_type_column()\n",
    "df_cleaned = type_cleaner.get_cleaned_data()  # DataFrame con \"type\" limpio\n",
    "\n",
    "# 5️⃣ Mostrar valores únicos de 'type' después de la limpieza\n",
    "if \"type\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'type' después de la limpieza:\")\n",
    "    print(df_cleaned[\"type\"].unique())\n",
    "\n",
    "# 6️⃣ Mostrar valores únicos de \"sex\" antes de la limpieza\n",
    "if \"sex\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'sex' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"sex\"].unique())\n",
    "\n",
    "# 4️⃣ Limpiar la columna \"sex\"\n",
    "sex_cleaner = SexCleaner(df_cleaned)\n",
    "sex_cleaner.clean_sex_column()\n",
    "df_cleaned = sex_cleaner.get_cleaned_data()  # DataFrame con \"sex\" limpio\n",
    "\n",
    "# 8️⃣ Mostrar valores únicos de \"sex\" después de la limpieza\n",
    "if \"sex\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'sex' después de la limpieza:\")\n",
    "    print(df_cleaned[\"sex\"].unique())\n",
    "\n",
    "# 9️⃣ Mostrar valores únicos de \"country\" antes de la limpieza\n",
    "if \"country\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'country' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"country\"].nunique())\n",
    "\n",
    "# 7️⃣ Limpiar la columna \"country\"\n",
    "country_cleaner = CountryCleaner(df_cleaned)\n",
    "country_cleaner.clean_country_column()\n",
    "df_cleaned = country_cleaner.get_cleaned_data()  # DataFrame con \"country\" limpio\n",
    "\n",
    "# 10️⃣ Mostrar valores únicos de 'country' después de la limpieza\n",
    "if \"country\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'country' después de la limpieza:\")\n",
    "    print(df_cleaned[\"country\"].nunique())\n",
    "\n",
    "# 11️⃣ Mostrar valores únicos de \"fatal_y/n\" antes de la limpieza\n",
    "if \"fatal_y/n\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'fatal_y/n' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"fatal_y/n\"].unique())\n",
    "\n",
    "# 12️⃣ Limpiar la columna \"fatal_y/n\"\n",
    "fatal_cleaner = FatalCleaner(df_cleaned)\n",
    "fatal_cleaner.clean_fatal_column()\n",
    "df_cleaned = fatal_cleaner.get_cleaned_data()  # DataFrame con \"fatal\" limpio\n",
    "\n",
    "# 13️⃣ Mostrar valores únicos de 'fatal' después de la limpieza\n",
    "if \"fatal\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'fatal' después de la limpieza:\")\n",
    "    print(df_cleaned[\"fatal\"].unique())\n",
    "\n",
    "# 14️⃣ Mostrar valores únicos de \"time\" antes de la limpieza\n",
    "if \"time\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'time' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"time\"].nunique())\n",
    "\n",
    "# 15️⃣ Limpiar la columna \"time\"\n",
    "time_cleaner = TimeCleaner(df_cleaned)\n",
    "time_cleaner.clean_time_column()\n",
    "df_cleaned = time_cleaner.get_cleaned_data()  # DataFrame con \"time\" limpio\n",
    "\n",
    "# 16️⃣ Mostrar valores únicos de 'time' después de la limpieza\n",
    "if \"time\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'time' después de la limpieza:\")\n",
    "    print(df_cleaned[\"time\"].nunique())\n",
    "\n",
    "# 17️⃣ Mostrar valores únicos de \"species\" antes de la limpieza\n",
    "if \"species\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'species' antes de la limpieza:\")\n",
    "    print(df_cleaned[\"species\"].unique())\n",
    "\n",
    "# 18️⃣ Limpiar la columna \"species\"\n",
    "species_cleaner = SpeciesCleaner(df_cleaned)\n",
    "species_cleaner.clean_species_column()\n",
    "df_cleaned = species_cleaner.get_cleaned_data()  # DataFrame con \"species\" limpio\n",
    "\n",
    "# 19️⃣ Mostrar valores únicos de 'species' después de la limpieza\n",
    "if \"species\" in df_cleaned.columns:\n",
    "    print(\"\\n🔹 Valores únicos de 'species' después de la limpieza:\")\n",
    "    print(df_cleaned[\"species\"].unique())\n",
    "\n",
    "# 5️⃣ Eliminar duplicados en \"case_number\"\n",
    "duplicates_cleaner = DuplicatesCleaner(df_cleaned)\n",
    "duplicates_cleaner.remove_duplicates()\n",
    "df_final = duplicates_cleaner.get_cleaned_data()  # DataFrame sin duplicados\n",
    "\n",
    "# 6️⃣ Guardar los datos procesados con nombres de columnas, \"type\", \"sex\", \"country\", \"fatal\", \"time\", \"species\" y sin duplicados\n",
    "duplicates_cleaner.save_cleaned_data(output_file)\n",
    "\n",
    "markdown_table = df_final.head(10).to_markdown()\n",
    "print(markdown_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
